---
layout: default
permalink: /teaching/adls/
---

# Advanced Deep Learning Systems (ADLS)

Course code: ELEC70109/EE9-AML3-10/EE9-AO25

---

## Important dates

- ~~7th Feb~~ 14th Feb (extended), Practical submission
- 9th Feb, Team project sign-up ends
- ~~10th Feb~~ 17th Feb (extended), Midterm lab oral
- 17th Mar, Show me your code session
- 27th Mar, Report submission and pull request finalization

---

## Full dates

| Week Number| Date       | Schedule    |
|------------|------------|-------------|
| 2          | 16th Jan   | Lectures (1 and 2, Room 407A, 11am-1pm), Team signup starts		|
| 3          | 20nd Jan 	| Lectures (3 and 4, Room 508B, 10am-noon) and lab sessions (Room 407, 4pm-6pm)	|
| 4          | 27th Jan 	| lab session (Room 407A, 4pm-6pm)			 										|	
| 5          | 3rd  Feb  	| lab session (Room 407A, 4pm-6pm)		   									|
| 5          | 6th  Feb 	| lab session (Room 407A, 11am-1pm)					 							|
| 6          | 10th Feb 	| lab session (extra) (Room 403B, 4pm-6pm)							  				|
| 6          | 13th Feb 	| Lectures (5 and 6, Room 407A, 11am-1pm)				 							|
| 7          | 17th Feb 	| Lectures (7 and 8, Room 508B, 10am-noon)				 							|
| 7          | 17th Feb 	| Lab oral (extended ddl, room and time tbc)				 							|
| 7          | 20th Feb 	| Lectures (9 and 10, Room 407A, 11am-1pm)										|
| 8          | 24th Feb 	| Lectures (11 and 12, Room 407A, 11am-1pm) 									|
| 8          | 27th Feb 	| Team Project Lab (Project scoping) 		|
| 9          | 3rd Mar 	  | Team Project Lab 											|
| 10         | 10th Mar 	| Team Project Lab 											|
| 11         | 17th Mar 	| Show me your code 										|
| End of term| 27th Mar 	| Final report and code submission 			|

---

#### Lecture 1: Introduction

- <a href="../../assets/pdf/adls_2025/lecture1.pdf">Slides</a>
- [Git tutorial](https://jianyicheng-research.notion.site/Git-Tutorial-516864ab8fa04242ad520652744b931f)
- [Dark Silicon and the End of Multicore Scaling](https://research.cs.wisc.edu/vertical/papers/2011/isca11-darksilicon.pdf)
- [Managing Wire Delay in Large Chip-Multiprocessor Caches](https://ieeexplore.ieee.org/abstract/document/1551004?casa_token=P5sarPuvBZ4AAAAA:eh8TDWxx89Z04mkFw2KdFrvWhD2raDe_u66ES8e5ZEpxq276zQ0wfs2uE6tWVdQhodRf9lSmAQ)

#### Lecture 2: MASE: Abstractions, Optimizations and Implementations

- <a href="../../assets/pdf/adls_2025/lecture2.pdf">Slides</a>
- [Troch FX](https://pytorch.org/docs/stable/fx.html)
- [MASE](https://github.com/DeepWok/mase)

#### Lecture 3: An Introduction to Labs

- <a href="../../assets/pdf/adls_2025/lecture3.pdf">Slides</a>

#### Lecture 4: An Introduction to Labs (2)

- <a href="../../assets/pdf/adls_2025/lecture4.pdf">Slides</a>


#### Lecture 5: Understanding the workload

<!-- - <a href="../../assets/pdf/adls/lecture5.pdf">Slides</a> -->
- [Convolution and DeConvolution animation](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)
- [Convolution padding and striding animation](https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html)
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- [Deconvolutional Networks](https://ieeexplore.ieee.org/document/5539957)
- [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)
- [Attention Is All You Need](<https://arxiv.org/abs/1706.03762>)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](<https://arxiv.org/abs/1810.04805>)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](<https://arxiv.org/abs/1910.10683>)

#### Lecture 6: Understanding the workload (2)

<!-- - <a href="../../assets/pdf/adls/lecture6.pdf">Slides</a> -->
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [Segment Anything](https://arxiv.org/abs/2304.02643)
- [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
- [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)

#### Lecture 7: Architectural Optimizations

<!-- - <a href="../../assets/pdf/adls/lecture7.pdf">Slides</a> -->
- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
- [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)
- [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

#### Lecture 8: Automated Machine Learning â€“ An Introduction to Network Architecture Search

<!-- - <a href="../../assets/pdf/adls/lecture8.pdf">Slides</a> -->
- [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)
- [Exploring Randomly Wired Neural Networks for Image Recognition](https://arxiv.org/abs/1904.01569)
- [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)
- [Probabilistic Dual Network Architecture Search on Graphs](https://arxiv.org/abs/2003.09676)
- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)
- [Neural Architecture Search without Training](https://arxiv.org/abs/2006.04647)
- [Speedy Performance Estimation for Neural Architecture Search](https://arxiv.org/abs/2006.04492)

#### Lecture 9: Network Compression

<!-- - <a href="../../assets/pdf/adls/lecture9.pdf">Slides</a> -->
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [SNIP: Single-shot Network Pruning based on Connection Sensitivity](https://arxiv.org/abs/1810.02340)
- [Progressive Skeletonization: Trimming more fat from a network at initialization](https://arxiv.org/abs/2006.09081)

#### Lecture 10: Network Compression (2)

<!-- - <a href="../../assets/pdf/adls/lecture10.pdf">Slides</a> -->
- [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
- [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://arxiv.org/abs/1612.03928)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)
- [EIE: Efficient Inference Engine on Compressed Deep Neural Network](https://arxiv.org/abs/1602.01528)

#### Lecture 11: Computation Graph and Operator-level Optimization

<!-- - <a href="../../assets/pdf/adls/lecture11.pdf">Slides</a> -->
- [TASO: The Tensor Algebra SuperOptimizer for Deep Learning](https://github.com/jiazhihao/TASO)
- [Tensorflow Grappler](https://www.tensorflow.org/guide/graph_optimization)
- [TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://arxiv.org/abs/1802.04799)
- [MLIR](https://mlir.llvm.org/)


#### Lecture 12: Distributed Training and Inference
<!-- - <a href="../../assets/pdf/adls/lecture12.pdf">Slides</a> -->
- [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://arxiv.org/abs/2201.12023)
- [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)
- [Horovod: fast and easy distributed deep learning in TensorFlow](https://arxiv.org/abs/1802.05799)



## Labs

Labs are hosted on [MASE Wiki](https://deepwok.github.io/mase/modules/adls_2024.html)

## Assessment Information

The assessment contains practicals (30% in total), and a team project with two people (70%).
If you used Colab Pro, we are wiling to reimburse that cost, please keep a copy of your receipt.

##### Practical Submission (30%)

- Deadline (7th Feb, email to a.zhao@imperial.ac.uk): **A Markdown file**: with all answers (plots, tables ...) of the questions and optional questions in labs. The file should be named as `adls_labs_your_college_short_code.md` (eg. `adls_labs_yz10513.md`).
- Corresponding code in your forked repository (will be checked in the lab oral, no submission needed). But you can include code snippets in the submitted markdown file.
- Deadline (10th Feb) **Lab Oral**.

##### Team Project (70%)

- A team of 3-4 people
- [Team making link](https://docs.google.com/spreadsheets/d/1zg-qIOXIge9w0x10YjUymkymHlJOjiGJqY_3cFd7oRs/edit?usp=sharing)
- [Team Project Requirements](https://docs.google.com/document/d/17XR49GnwWnGpyfBIqOFSyk5p7dhRDmf-pPFy1-VB5FY/edit?usp=sharing)
- [Past project lists](https://docs.google.com/document/d/1rRIJLxIZMVM3DLf2M_sHNxKYqzXlHxA5vM8Rl5YHehI/edit?usp=sharing)
- A roadmap meeting with a GTA (not assessed).
- Deadline (17th March): **A show me your code lab oral** (assessed).  
- Deadline (27th March): **Coursework code submission** as a pull request or an open repository on Github (assessed). The PR or repository should also have a clear documentation of the functionality that you have implemented and its corresponding tests. There should also be testing in place.
- Deadline (27th March, PDF attached in the PR): **6-page Report** (assessed). The report should mainly focus on an evaluation of your implementation. The description of functionality and testing have being described in the PR or code repository already, so this 6-page report should include only the experiments you have designed to explore certain trade-offs and properties of your optimization.